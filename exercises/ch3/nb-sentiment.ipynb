{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook set deals with IMDB review data (from [here](https://ai.stanford.edu/~amaas/data/sentiment/)).  The dataset has been adjusted slightly for the purposes of this exercise to conform to the description that follows and to limit the total number of reviews.\n",
    "\n",
    "The goal is to explore using naive Bayes to predict whether a review is negative (bad) or positive (good) from the frequency with which different words appear in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are the only libraries you should need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# below line just to make figures larger\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset consists of reviews with either bad (<=4) or good (>=7) ratings.  Each data point is a different movie review.  The label, $y$, is $0$ if the review is a bad review and $1$ if the review is a good review.\n",
    "\n",
    "There are 1000 features, corresponding the the 1000 most common words across all reviews.  For example, the first feature corresponds to the word \"the\" and the second feature corresponds to the word \"and.\"  The value of the feature is an integer from 0 to 5, with 0 indicating this word does not appear in the review (\"none\"), 1 indicating the word appears once (\"once\"), 2 indicating the word appears fewer than 5 times (\"some\"), 3 indicating the word appears fewer than 10 times (\"few\"), 4 indicating the word appears fewer than 20 times (\"many\"), and 5 indicating the word appears 20 times or more (\"lots\").\n",
    "\n",
    "The code below loads in both a training dataset (`train`) and a testing dataset (`test`).  Most importantly, each as a `.X` and `.Y` field: numpy arrays of the $X$ and $Y$ matrices.  You can also find the words that correspond to each feature (`.featnames`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import loaddataset\n",
    "\n",
    "#adjust the directory below if you have moved either the dataset files or this notebook\n",
    "datasetdir = '../../datasets/' \n",
    "\n",
    "train = loaddataset(datasetdir+'sentiment-cat-train')\n",
    "test = loaddataset(datasetdir+'sentiment-cat-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "In the cell below, plot a grid of histograms (5 columns, 5 rows)\n",
    "Each histogram should be the distribution of a different feature (so you will be plotting just the first 25 features).  The histograms should have two sets of bars (each in their own color): ones for examples from class 0 and one for examples from class 1.  You want side-by-side histograms, each with 6 bars (for 12 bars in two colors). Be sure to give a title to each plot with the feature number and the corresponding word.  Use only the training data for these histograms.\n",
    "    \n",
    "matplotlib's [subplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html) and [hist](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) functions may be helpful.\n",
    "    \n",
    "As an example, the histogram in the upper left, corresponding to feature 0, should look like\n",
    "\n",
    "![x0 histogram](./histo0.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    \n",
    "<b>part a</b> For the 25 features above, based on the histograms you plotted, which would the most helpful three features for classifying this dataset using naive Bayes?  Which would be the least helpful feature?  <b>WHY?</b>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    \n",
    "<b>part b</b> Suggest a way to quantify how good a feature is.  Implement it, and list the three best and three worst feature **out of all 1000** features\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "        \n",
    "Complete the two functions in the two cells below.\n",
    "\n",
    "The first trains/learns a naive Bayes classifier.  The second predicts the classes for a set of examples based on the model learned.\n",
    "    \n",
    "Hint:  Test your code on a simpler example where you know the answer.  For instance, the data from Table 3.2 (which can be found as the dataset `tbl3-2`)\n",
    "    \n",
    "Hint 2: You will have to deal with counts that are 0.  Leaving them as zero will result in 0 probabilities that will cause problems.  A standard way to handle this is to add 1 to all counts (this is often called Laplace smoothing).  For frequent feature values, it does not change things much.  Do this separately for each table before normalizing.  For infrequent values, it keeps them away from 0 and admits that they might happen more often than was seen in the data.  This is like pretending there are extra examples that cause the raw counts to increase by 1.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnnb(ds):\n",
    "    # ds.X is shape (m,n) (m data points, each with n features).  Its elements are integer values from 0 to numfeatvals-1 (inclusive)\n",
    "    # ds.Y is shape (m,) (m data points).  It has values of either 0 or 1 (class \"0\" or class \"1\")\n",
    "    numfeatvals = len(ds.featcats[0]) # assuming all features have the same number of possible values\n",
    "    \n",
    "    # this function is to return a pair (priorp,condp)\n",
    "    # where priorp is of shape (2,) and has the prior probability of each of the two classes\n",
    "    #  and  condp is of shape (n,numfeatvals,2) and has the conditional probabilities for the naive Bayes classifier\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prednb(X,model,uselog=True):\n",
    "    # X is of shape (m,n) (m data points, each with n features).\n",
    "    # model is the pair (priorp,condp), as returned from learnnb\n",
    "    # should return something of shape (m,) which is an array of 0s and 1s, indicating\n",
    "    # the predicted (most probable under NB) class for each of the examples in X\n",
    "    (priorp,condp) = model\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "The code below trains a naive Bayes classifier and then tests it on the testing examples and reports the error rate.\n",
    "    \n",
    "Run the code to report the testing error.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorrate(predY,trueY):\n",
    "    if len(predY.shape)>1:\n",
    "        predY = predY[:,0]\n",
    "    if len(trueY.shape)>1:\n",
    "        trueY = trueY[:,0]\n",
    "    return (predY!=trueY).mean()\n",
    "\n",
    "model = learnnb(train)\n",
    "predY = prednb(test.X,model)\n",
    "print(errorrate(predY,test.Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
