{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook set deals with IMDB review data (from [here](https://ai.stanford.edu/~amaas/data/sentiment/)).  The dataset has been adjusted slightly for the purposes of this exercise to conform to the description that follows and to limit the total number of reviews.\n",
    "\n",
    "The goal is to explore using linear least squares to predict a moview review's rating from the frequency with which different words appear in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are the only libraries you should need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# below line just to make figures larger\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset consists of reviews with their corresponding ratings.  Each data point is a different movie review.  The label, $y$, is the review's rating (from 1 to 10, with 10 being the best).\n",
    "\n",
    "There are 1000 features, corresponding the the 1000 most common words across all reviews.  For example, the first feature corresponds to the word \"the\" and the second feature corresponds to the word \"and.\"  The value of the feature is the nubmer of times that word appears in the review.  **Note: This is different from the dataset used for naive Bayes: the features and target are both real-valued.**\n",
    "\n",
    "The code below loads in both a training dataset (`train`) and a testing dataset (`test`).  Most importantly, each as a `.X` and `.Y` field: numpy arrays of the $X$ and $Y$ matrices.  You can also find the words that correspond to each feature (`.featnames`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../..') # path to dataset.py (adjust if you have moved this file or dataset.py)\n",
    "from dataset import loaddataset\n",
    "\n",
    "#adjust the directory below if you have moved either the dataset files or this notebook\n",
    "datasetdir = '../../datasets/' \n",
    "\n",
    "train = loaddataset(datasetdir+'moviereview-reg-train')\n",
    "test = loaddataset(datasetdir+'moviereview-reg-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "Complete the training and testing functions below for linear least squares.  You code should <b>not</b> add the extra \"constant\" feature.  Assume that is done already.  These functions are very short.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnlls(X,Y):\n",
    "    # X is the data matrix of shape (m,n)\n",
    "    # Y is are the target values of shape (m,)\n",
    "    # function should return w of shape (n,)        \n",
    "    return np.linalg.solve((X.T@X),X.T@Y)\n",
    "    \n",
    "def predictlls(X,w):\n",
    "    # X is the (testing) data of shape (m,n)\n",
    "    # w are the weights learned in linear least-squares regression of shape (n,)\n",
    "    # function should return Y, the predicted values of shape (m,)\n",
    "    return X@w\n",
    "    \n",
    "def testlls(X,Y,w):\n",
    "    # X and Y are the testing data\n",
    "    # w are the weights from linear least-squares regression\n",
    "    # returns the *mean* squared error\n",
    "    Ydelta = Y - predictlls(X,w)\n",
    "    return (Ydelta*Ydelta).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    \n",
    "Using the functions above, compute the weights learned using linear least squares on the training data.  Then report the mean squared error of the resulting regressor on the training data and the testing data.\n",
    "\n",
    "You will need to augment the features with an initial column of all 1s to allow for an offset for the learned function.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 5.699668546224271\n",
      "testing error: 6.2007346197200075\n"
     ]
    }
   ],
   "source": [
    "def add1s(X):\n",
    "    return np.hstack((np.ones((X.shape[0],1)),X))\n",
    "\n",
    "w = learnlls(add1s(train.X),train.Y)\n",
    "trainerr = testlls(add1s(train.X),train.Y,w)\n",
    "testerr = testlls(add1s(test.X),test.Y,w)\n",
    "\n",
    "print(f'training error: {trainerr}')\n",
    "print(f'testing error: {testerr}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
